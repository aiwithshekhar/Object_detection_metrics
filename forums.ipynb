{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting sub part of tensor and the indices of selected elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "from torch import FloatTensor\n",
    "from torch.autograd import Variable\n",
    "import shutil\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models import resnet18\n",
    "from graphviz import Digraph\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.randint(0, 10, (10, 10))\n",
    "idx = (A > 5).nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list of numbers to Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_val=[2,5,7,3,2,4,2]\n",
    "# list_val=torch.tensor(list_val)\n",
    "# list_val=torch.Tensor(list_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output from network varies depending on CPU or GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for 3d array axis=0 means sum along channel, for 2d array sum=0 means sum along rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference in CPU architecture between both posted CPUs might explain the difference\n",
    "\n",
    "# Also, the implementation of the pseudo-random number generator might be different \n",
    "# for different hardware devices (e.g. CPU vs. GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((4, 3, 3))\n",
    "s1 = x.sum()\n",
    "s2 = x.sum(0).sum(0).sum(0)\n",
    "print(s1 - s2,(s1 - s2).abs(),(s1 - s2).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ValueError: canâ€™t optimize a non leaf Tensor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradients can only be stored for leaf nodes, leaf nodes are those which are independent \n",
    "# and not the result of some comtutation like a, w1,w2,w3,w4 in below graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='/home/persistance/Pictures/Screenshot from 2020-02-08 00-20-08.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(10, requires_grad=True) # a is a leaf variable\n",
    "# a = torch.rand(10, requires_grad=True).double() # a is NOT a leaf variable as it was created by the operation that cast a float tensor into a double tensor\n",
    "# a = torch.rand(10).requires_grad_().double() # equivalent to the formulation just above: not a leaf variable\n",
    "# a = torch.rand(10).double() # a does not require gradients and has not operation creating it (tracked by the autograd engine).\n",
    "# a = torch.rand(10).double().requires_grad_() # a requires gradients and has no operations creating it: it's a leaf variable and can be given to an optimizer.\n",
    "# a = torch.rand(10, requires_grad=True, device=\"cuda\") # a requires grad, has not operation creating it: it's a leaf variable as well and can be given to an optimizer\n",
    "\n",
    "a.sum().backward()\n",
    "\n",
    "print (a.grad,a.is_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using bwlow formula we need to create the graph\n",
    "# b = w1 * a\n",
    "# c = w2 * a \n",
    "# d = (w3 * b) + (w4 * c)\n",
    "# L = f(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=Variable(FloatTensor([4]))\n",
    "weights= [Variable(FloatTensor([i]), requires_grad=True) for i in (2,5,9,7)]\n",
    "w1,w2,w3,w4= weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch uses dynaic graph, when forwad is called then this graph is created as \n",
    "# we call backward gradiets are stored in the leaf node and this grap is destroyed. Calling two times backward\n",
    "# we get below error\n",
    "\n",
    "# RuntimeError: Trying to backward through the graph a second time,\n",
    "# but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=w1*a\n",
    "b.retain_grad()\n",
    "c=w2*a\n",
    "d= (w3*b) + (w4*c)\n",
    "L=10-d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L.backward()\n",
    "# L.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind,wei in enumerate(weights):\n",
    "    grad=wei.grad.data\n",
    "    print (f'grads are w{ind} {grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to add a pretrained model into a new model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = models.resnet50(pretrained=True)\n",
    "num_ftrs = resnet.fc.in_features\n",
    "print(num_ftrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet.fc=nn.Identity()\n",
    "inp=torch.randn(1,3,214,214)\n",
    "out=resnet(inp)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinetuneResnet(nn.Module):\n",
    "    def __init__(self, resnet_pretrain, resnet_filter, num_classes):\n",
    "        super(FinetuneResnet, self).__init__()\n",
    "        self.resnet = resnet_pretrain\n",
    "        self.fc1 = nn.Linear(resnet_filter, 2048)\n",
    "        self.fc2 = nn.Linear(2048, num_classes)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.softmax(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FinetuneResnet(resnet, num_ftrs, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic unet architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    def __init__(self,inp,out):\n",
    "        super().__init__()\n",
    "        self.conv1=self._convolve(inp,16)\n",
    "        self.conv2=self._convolve(16,32)\n",
    "        self.conv3=self._convolve(32,64)\n",
    "        \n",
    "        self.maxpool=nn.MaxPool2d(2)\n",
    "        self.upsample=nn.Upsample(scale_factor=2,mode='bilinear',align_corners=True)\n",
    "        \n",
    "        self.conv_decode1=self._convolve(64+32,32)\n",
    "        self.conv_decode2=self._convolve(32+16,16)\n",
    "        self.final_conv=nn.Conv2d(16,1,1)\n",
    "    \n",
    "    def _convolve(self,inp,out,kernel=3):\n",
    "        \n",
    "        block=nn.Sequential(nn.Conv2d(inp,out,kernel_size=kernel,padding=1),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            nn.BatchNorm2d(out),\n",
    "                            nn.Conv2d(out,out,kernel_size=kernel,padding=1),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            nn.BatchNorm2d(out))\n",
    "        return block\n",
    "    \n",
    "    def forward(self,x):\n",
    "        conv1 = self.conv1(x)\n",
    "        x = self.maxpool(conv1)\n",
    "        conv2 = self.conv2(x)\n",
    "        x = self.maxpool(conv2)\n",
    "        conv3 = self.conv3(x)\n",
    "        x = self.upsample(conv3)\n",
    "        x=torch.cat([x,conv2],axis=1)\n",
    "        x=self.conv_decode1(x)\n",
    "        x=self.upsample(x)\n",
    "        x=torch.cat([x,conv1], axis=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=Unet(1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpput_u=torch.randn(1,100,100)\n",
    "inpput_u=inpput_u.unsqueeze(0)\n",
    "inpput_u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=net(inpput_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An nn.Module class helps in managing the internal parameters and makes some workflows easier,\n",
    "# e.g. pushing all parameters to the GPU by calling .to('cuda')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to change dtypes\n",
    "ran=torch.randn(3,4).half()\n",
    "ran.dtype\n",
    "\n",
    "model.to(torch.double) #will convert all parameters to float64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving & loading files in pytorch for resuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *******torch.save(the_model.state_dict(), PATH)\n",
    "# *******the_model = TheModelClass(*args, **kwargs)\n",
    "# *******the_model.load_state_dict(torch.load(PATH))\n",
    "# only its parameters get pickled.\n",
    "\n",
    "# *******torch.save(the_model, PATH)\n",
    "# *******the_model = torch.load(PATH)\n",
    "# Here whole object gets pickled,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc1 = 0.78   #starts with 0 and keep on updating if current accuracy is greater tha previous one.\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.001)\n",
    "acc1 = 0.62  #validation accuracy\n",
    "\n",
    "is_best = acc1 > best_acc1\n",
    "best_acc1 = max(acc1, best_acc1)\n",
    "\n",
    "save_checkpoint({\n",
    "    'epoch': 1,\n",
    "    'arch': net,\n",
    "    'state_dict': net.state_dict(),\n",
    "    'best_prec1': best_acc1,\n",
    "    'optimizer' : optimizer.state_dict(),\n",
    "}, is_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('checkpoint.pth.tar'):\n",
    "    print(\"=> loading checkpoint '{}'\".format('checkpoint.pth.tar'))\n",
    "    checkpoint = torch.load('checkpoint.pth.tar')\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    best_prec1 = checkpoint['best_prec1']\n",
    "    net.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "          .format('checkpoint.pth.tar', checkpoint['epoch']))\n",
    "else:\n",
    "    print(\"=> no checkpoint found at '{}'\".format('checkpoint.pth.tar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight initialization pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = models.resnet34(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet.conv1.weight.mean(), resnet.conv1.weight.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.kaiming_normal_(m.weight.data)\n",
    "\n",
    "def weights_init1(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet.apply(weights_init);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet.conv1.weight.mean(), resnet.conv1.weight.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the architecture of existing models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet=resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifer(nn.Module):\n",
    "    def __init__(self, arc):\n",
    "        super().__init__()\n",
    "        self.new_classifer=nn.Sequential(*list(arc.children())[:-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        out=self.new_classifer(x)\n",
    "        out=out.view(-1,out.numel())\n",
    "        out=nn.Linear(512,1000)(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=Classifer(resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp=torch.randn(3,214,214)\n",
    "inp=inp.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=net(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without using hooks we can save intermediate weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, submodule, extracted_layers):\n",
    "        super().__init__()\n",
    "        self.submodule = nn.Sequential(*list(submodule.children())[:-1])\n",
    "        self.extracted_layers=extracted_layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "#         print (self.submodule.named_modules)\n",
    "        for name, module in self.submodule._modules.items():\n",
    "            x = module(x)\n",
    "#             print (name)\n",
    "            if name in self.extracted_layers:\n",
    "                outputs += [x]\n",
    "        return outputs + [x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tes=FeatureExtractor(resnet,['conv1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tes1=tes(inp);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_dot(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# swap axis in pytorch\n",
    "# transpose only applies to 2 axis, while permute can be applied to all the axes at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 1, 2])\n",
      "torch.Size([4, 3, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "a=torch.rand(1,2,3,4)\n",
    "a_permute=a.permute(3,2,0,1)\n",
    "b_trans=a.transpose(0,3).transpose(1,2).transpose(2,3)\n",
    "print (a_permute.shape)\n",
    "print (b_trans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crating one hot encoding vector, pytorch version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = torch.tensor([0, 1, 2, 3])\n",
    "torch.zeros(len(idx), idx.max()+1).scatter_(1, idx.unsqueeze(1), 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3), tensor(3))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values, indices = idx.max(0)\n",
    "values, indices = torch.max(idx, 0)\n",
    "values, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
